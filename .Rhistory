library(tidyverse, quietly = TRUE)
library(mda802)
library(MASS)
library(FactoMineR)
library(factoextra)
filenames <- list.files(pattern = "[[:alnum:]]-train")
filenames
set_train=do.call("rbind", lapply(filenames, read.csv, header = TRUE ,sep=";", dec=","))
set_train
don=read.csv("train.csv",sep=";")
don=read.csv("train.csv",sep=";")
don_train=read.csv("train.csv",sep=";")
don_test=read.csv("test.csv",sep=";")
don_train
don_train=read.csv("train.csv",sep=",")
don_test=read.csv("test.csv",sep=",")
don_train
res_pca <- PCA(don_train[,9:ncol(set_train)], scale.unit = TRUE, graph = T, ncp=5)
res_pca <- PCA(don_train[,9:ncol(don_train)], scale.unit = TRUE, graph = T, ncp=5)
res_pca <- PCA(don_train[,12:ncol(don_train)], scale.unit = TRUE, graph = T, ncp=5)
res_pca
res_pca$eig
karlis_saporta_spinaki=1+2*sqrt((ncol(don_train)-1)/(nrow(don_train)-1))
karlis_saporta_spinaki
fviz_contrib(res_pca, choice = "ind", axes = 1)
fviz_contrib(res_pca, choice = "ind", axes = 2)
fviz_contrib(res_pca, choice = "ind", axes = 3)
fviz_contrib(res_pca, choice = "ind", axes = 3)
fviz_pca_var(res_pca, axes=c(1,2), repel = TRUE)
getwd()
library(data.table)
library(Matrix)
library(xgboost)
library(caret)
library(dplyr)
cat("Read data")
df_train <- fread('data/train.csv', sep=",", na.strings = "NA")
df_test  <- fread("data/test.csv" , sep=",", na.strings = "NA")
data = rbind(df_train,df_test,fill=T)
features = colnames(data)
features
print(sapply(d_train,class))
d_train <- fread('data/train.csv', sep=",", na.strings = "NA")
d_test  <- fread("data/test.csv" , sep=",", na.strings = "NA")
print(sapply(d_train,class))
err_rate <-  function(D,prediction){
#matrice de confusion
mc <- table(D$chiffre,prediction)
#taux d’erreur
#1 -  somme(individus classés correctement) / somme totale individus
err <-1-sum(diag(mc))/sum(mc)
print(paste("Error rate :",round(100*err,2),"%"))
}
err_rate
View(err_rate)
install.packages(rpart)
install.packages('rpart')
library(rpart)
m.tree <-rpart(chiffre ~ ., data = dtrain)
m.tree <-rpart(chiffre ~ ., data = d_train)
m.tree <-rpart(y ~ ., data = d_train)
print(m.tree)
print(m.tree$variable.importance)
View(d_train)
d_train[0]
d_train[1]
d_train[,1]
sapply(d_train[,3:378],sd)
sapply(d_train[,11:378],sd)#comme les 8 premiere variable sont des factors alors nous allons les enlever
y.tree <-  predict(m.tree, newdata = d_test, type = "class")
View(d_test)
lm(y~.,data = d_train)
model_reglin=lm(y~.,data = d_train)
summary(model_reglin)
predict(object, newdata, se.fit = FALSE, scale = NULL, df = Inf,
interval = c("none", "confidence", "prediction"),
level = 0.95, type = c("response", "terms"))
predict(model_reglin(d_test), newdata, se.fit = FALSE, scale = NULL, df = Inf,
interval = c("none", "confidence", "prediction"),
level = 0.95, type = c("response", "terms"))
model_reglin(d_test)
model_reglin=lm(y~.,data = d_train)
model_reglin(d_test)
predict(model_reglin, newdata, se.fit = FALSE, scale = NULL, df = Inf,
interval = c("none", "confidence", "prediction"),
level = 0.95, type = c("response", "terms"))
predict(model_reglin, d_test, se.fit = FALSE, scale = NULL, df = Inf,
interval = c("none", "confidence", "prediction"),
level = 0.95, type = c("response", "terms"))
X0<-ifelse(XO)
X0<-ifelse(X0)
attach(d_train)
X0<-ifelse(X0)
X0
model_reglin=lm(y~.,data = d_train[,11:378])
summary(model_reglin)
predict(model_reglin, d_test[,10:378], se.fit = FALSE, scale = NULL, df = Inf,
interval = c("none", "confidence", "prediction"),
level = 0.95, type = c("response", "terms"))
predict(model_reglin, d_test[,10:377], se.fit = FALSE, scale = NULL, df = Inf,
interval = c("none", "confidence", "prediction"),
level = 0.95, type = c("response", "terms"))
y_pred=predict(model_reglin, d_test[,10:377], se.fit = FALSE, scale = NULL, df = Inf,
interval = c("none", "confidence", "prediction"),
level = 0.95, type = c("response", "terms"))
summary(y_pred)
err=d_train[2]-y_pred
d-train[2]
d_train[2]
err=d_train[,2]-y_pred
err
mean(err)
mean(as.numeric(err))
is.numeric(err)
as.numeric(err)
View(err)
err[0]
err[1]
err[,1]
err[,0]
mean(err[,1])
