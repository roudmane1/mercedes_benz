{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Objectives: \n",
    "#Nous allons parcourir la base de données de Mercedes Benz \n",
    "#Cette base de données contient les différentes options des voitures constuites,\n",
    "#C'est variables sont d'écrites sous forme de variables anonymes\n",
    "#La variable Y représente le temps qu'une voiture passe dans la borne de test \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anacanda\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# importer ici les librairies dont vous aurez besoin\n",
    "\n",
    "# pandas\n",
    "import pandas as pd\n",
    "from pandas import Series,DataFrame\n",
    "\n",
    "# numpy, matplotlib, seaborn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "%matplotlib inline\n",
    "\n",
    "# machine learning\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "from sklearn.pipeline import make_pipeline, make_union\n",
    "from sklearn.base import BaseEstimator,TransformerMixin, ClassifierMixin\n",
    "from sklearn.linear_model import ElasticNetCV, LassoLarsCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.utils import check_array\n",
    "\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class StackingEstimator(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, estimator):\n",
    "        self.estimator = estimator\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        self.estimator.fit(X, y, **fit_params)\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        X = check_array(X)\n",
    "        X_transformed = np.copy(X)\n",
    "        # add class probabilities as a synthetic feature\n",
    "        if issubclass(self.estimator.__class__, ClassifierMixin) and hasattr(self.estimator, 'predict_proba'):\n",
    "            X_transformed = np.hstack((self.estimator.predict_proba(X), X))\n",
    "\n",
    "        # add class prodiction as a synthetic feature\n",
    "        X_transformed = np.hstack((np.reshape(self.estimator.predict(X), (-1, 1)), X_transformed))\n",
    "\n",
    "        return X_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Récuperation des données, dimension des tableaux et "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#récuperation des données \n",
    "#données pour le train\n",
    "data_train = pd.read_csv(\"data/train.csv\")\n",
    "data_test = pd.read_csv(\"data/test.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4209, 378)\n",
      "on a, dans notre jeu de données, 4209 voitures, 378 options variables \n"
     ]
    }
   ],
   "source": [
    "#dimension du train \n",
    "data_train.shape\n",
    "print(data_train.shape)\n",
    "print(\"on a, dans notre jeu de données, 4209 voitures, 378 options variables \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4209"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dimension du test \n",
    "data_test.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>y</th>\n",
       "      <th>X0</th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X8</th>\n",
       "      <th>...</th>\n",
       "      <th>X375</th>\n",
       "      <th>X376</th>\n",
       "      <th>X377</th>\n",
       "      <th>X378</th>\n",
       "      <th>X379</th>\n",
       "      <th>X380</th>\n",
       "      <th>X382</th>\n",
       "      <th>X383</th>\n",
       "      <th>X384</th>\n",
       "      <th>X385</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>130.81</td>\n",
       "      <td>k</td>\n",
       "      <td>v</td>\n",
       "      <td>at</td>\n",
       "      <td>a</td>\n",
       "      <td>d</td>\n",
       "      <td>u</td>\n",
       "      <td>j</td>\n",
       "      <td>o</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>88.53</td>\n",
       "      <td>k</td>\n",
       "      <td>t</td>\n",
       "      <td>av</td>\n",
       "      <td>e</td>\n",
       "      <td>d</td>\n",
       "      <td>y</td>\n",
       "      <td>l</td>\n",
       "      <td>o</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>76.26</td>\n",
       "      <td>az</td>\n",
       "      <td>w</td>\n",
       "      <td>n</td>\n",
       "      <td>c</td>\n",
       "      <td>d</td>\n",
       "      <td>x</td>\n",
       "      <td>j</td>\n",
       "      <td>x</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>80.62</td>\n",
       "      <td>az</td>\n",
       "      <td>t</td>\n",
       "      <td>n</td>\n",
       "      <td>f</td>\n",
       "      <td>d</td>\n",
       "      <td>x</td>\n",
       "      <td>l</td>\n",
       "      <td>e</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13</td>\n",
       "      <td>78.02</td>\n",
       "      <td>az</td>\n",
       "      <td>v</td>\n",
       "      <td>n</td>\n",
       "      <td>f</td>\n",
       "      <td>d</td>\n",
       "      <td>h</td>\n",
       "      <td>d</td>\n",
       "      <td>n</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 378 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID       y  X0 X1  X2 X3 X4 X5 X6 X8  ...   X375  X376  X377  X378  X379  \\\n",
       "0   0  130.81   k  v  at  a  d  u  j  o  ...      0     0     1     0     0   \n",
       "1   6   88.53   k  t  av  e  d  y  l  o  ...      1     0     0     0     0   \n",
       "2   7   76.26  az  w   n  c  d  x  j  x  ...      0     0     0     0     0   \n",
       "3   9   80.62  az  t   n  f  d  x  l  e  ...      0     0     0     0     0   \n",
       "4  13   78.02  az  v   n  f  d  h  d  n  ...      0     0     0     0     0   \n",
       "\n",
       "   X380  X382  X383  X384  X385  \n",
       "0     0     0     0     0     0  \n",
       "1     0     0     0     0     0  \n",
       "2     0     1     0     0     0  \n",
       "3     0     0     0     0     0  \n",
       "4     0     0     0     0     0  \n",
       "\n",
       "[5 rows x 378 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print(data_train.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "#on peut voir qu'il y a pas de valeurs manquantes\n",
    "print(data_train.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>y</th>\n",
       "      <th>X0</th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X8</th>\n",
       "      <th>...</th>\n",
       "      <th>X375</th>\n",
       "      <th>X376</th>\n",
       "      <th>X377</th>\n",
       "      <th>X378</th>\n",
       "      <th>X379</th>\n",
       "      <th>X380</th>\n",
       "      <th>X382</th>\n",
       "      <th>X383</th>\n",
       "      <th>X384</th>\n",
       "      <th>X385</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>130.81</td>\n",
       "      <td>k</td>\n",
       "      <td>v</td>\n",
       "      <td>at</td>\n",
       "      <td>a</td>\n",
       "      <td>d</td>\n",
       "      <td>u</td>\n",
       "      <td>j</td>\n",
       "      <td>o</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>88.53</td>\n",
       "      <td>k</td>\n",
       "      <td>t</td>\n",
       "      <td>av</td>\n",
       "      <td>e</td>\n",
       "      <td>d</td>\n",
       "      <td>y</td>\n",
       "      <td>l</td>\n",
       "      <td>o</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>76.26</td>\n",
       "      <td>az</td>\n",
       "      <td>w</td>\n",
       "      <td>n</td>\n",
       "      <td>c</td>\n",
       "      <td>d</td>\n",
       "      <td>x</td>\n",
       "      <td>j</td>\n",
       "      <td>x</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>80.62</td>\n",
       "      <td>az</td>\n",
       "      <td>t</td>\n",
       "      <td>n</td>\n",
       "      <td>f</td>\n",
       "      <td>d</td>\n",
       "      <td>x</td>\n",
       "      <td>l</td>\n",
       "      <td>e</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13</td>\n",
       "      <td>78.02</td>\n",
       "      <td>az</td>\n",
       "      <td>v</td>\n",
       "      <td>n</td>\n",
       "      <td>f</td>\n",
       "      <td>d</td>\n",
       "      <td>h</td>\n",
       "      <td>d</td>\n",
       "      <td>n</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>18</td>\n",
       "      <td>92.93</td>\n",
       "      <td>t</td>\n",
       "      <td>b</td>\n",
       "      <td>e</td>\n",
       "      <td>c</td>\n",
       "      <td>d</td>\n",
       "      <td>g</td>\n",
       "      <td>h</td>\n",
       "      <td>s</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>24</td>\n",
       "      <td>128.76</td>\n",
       "      <td>al</td>\n",
       "      <td>r</td>\n",
       "      <td>e</td>\n",
       "      <td>f</td>\n",
       "      <td>d</td>\n",
       "      <td>f</td>\n",
       "      <td>h</td>\n",
       "      <td>s</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>25</td>\n",
       "      <td>91.91</td>\n",
       "      <td>o</td>\n",
       "      <td>l</td>\n",
       "      <td>as</td>\n",
       "      <td>f</td>\n",
       "      <td>d</td>\n",
       "      <td>f</td>\n",
       "      <td>j</td>\n",
       "      <td>a</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>27</td>\n",
       "      <td>108.67</td>\n",
       "      <td>w</td>\n",
       "      <td>s</td>\n",
       "      <td>as</td>\n",
       "      <td>e</td>\n",
       "      <td>d</td>\n",
       "      <td>f</td>\n",
       "      <td>i</td>\n",
       "      <td>h</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>30</td>\n",
       "      <td>126.99</td>\n",
       "      <td>j</td>\n",
       "      <td>b</td>\n",
       "      <td>aq</td>\n",
       "      <td>c</td>\n",
       "      <td>d</td>\n",
       "      <td>f</td>\n",
       "      <td>a</td>\n",
       "      <td>e</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 378 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID       y  X0 X1  X2 X3 X4 X5 X6 X8  ...   X375  X376  X377  X378  X379  \\\n",
       "0   0  130.81   k  v  at  a  d  u  j  o  ...      0     0     1     0     0   \n",
       "1   6   88.53   k  t  av  e  d  y  l  o  ...      1     0     0     0     0   \n",
       "2   7   76.26  az  w   n  c  d  x  j  x  ...      0     0     0     0     0   \n",
       "3   9   80.62  az  t   n  f  d  x  l  e  ...      0     0     0     0     0   \n",
       "4  13   78.02  az  v   n  f  d  h  d  n  ...      0     0     0     0     0   \n",
       "5  18   92.93   t  b   e  c  d  g  h  s  ...      0     0     1     0     0   \n",
       "6  24  128.76  al  r   e  f  d  f  h  s  ...      0     0     0     0     0   \n",
       "7  25   91.91   o  l  as  f  d  f  j  a  ...      0     0     0     0     0   \n",
       "8  27  108.67   w  s  as  e  d  f  i  h  ...      1     0     0     0     0   \n",
       "9  30  126.99   j  b  aq  c  d  f  a  e  ...      0     0     1     0     0   \n",
       "\n",
       "   X380  X382  X383  X384  X385  \n",
       "0     0     0     0     0     0  \n",
       "1     0     0     0     0     0  \n",
       "2     0     1     0     0     0  \n",
       "3     0     0     0     0     0  \n",
       "4     0     0     0     0     0  \n",
       "5     0     0     0     0     0  \n",
       "6     0     0     0     0     0  \n",
       "7     0     0     0     0     0  \n",
       "8     0     0     0     0     0  \n",
       "9     0     0     0     0     0  \n",
       "\n",
       "[10 rows x 378 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4209 entries, 0 to 4208\n",
      "Columns: 378 entries, ID to X385\n",
      "dtypes: float64(1), int64(369), object(8)\n",
      "memory usage: 12.1+ MB\n"
     ]
    }
   ],
   "source": [
    "data_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a 8 variables qualitatives et 369 variables boolein "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La variable Y est la variable temps passée par une voiture dans le branche de test pour chaque variable.\n",
    "c'est la variable qu'on va prédir après \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ID</td>\n",
       "      <td>int64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>y</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>X0</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>X1</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>X2</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>X3</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>X4</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>X5</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>X6</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>X8</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>X10</td>\n",
       "      <td>int64</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index        0\n",
       "0     ID    int64\n",
       "1      y  float64\n",
       "2     X0   object\n",
       "3     X1   object\n",
       "4     X2   object\n",
       "5     X3   object\n",
       "6     X4   object\n",
       "7     X5   object\n",
       "8     X6   object\n",
       "9     X8   object\n",
       "10   X10    int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtype_data = data_train.dtypes.reset_index()\n",
    "dtype_data.loc[:10,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#data avec la variable reponse et les variables qualitatives \n",
    "#data_quant=data_train[[\"y\",\"X0\",\"X1\",\"X2\",\"X3\",\"X4\",\"X5\",\"X6\",\"X8\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#data_quant[\"X10\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "corr = data_quant.corr() # matrix de correlation\n",
    "f, ax = plt.subplots(figsize=(11, 9))\n",
    "sns.heatmap(corr,annot=True, vmax=.3)\n",
    "plt.show()\n",
    "# on peut s'apercevoir sur ce heatmap des corrélation que la variable \"left\"\n",
    "# est le plus corrélé avec \"time_spend_compagny\" et average_monthly_hours\n",
    "# et est négativement corrélé avec les varibles \"satisfaction level \" et \"working_accident\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import seaborn as sns\n",
    "corr = data_uant.corr()\n",
    "sns.heatmap(corr, \n",
    "            xticklabels=corr.columns.values,\n",
    "            yticklabels=corr.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXkAAAECCAYAAAAb5qc/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAFupJREFUeJzt3X+QXWd93/H3rmXhApJNgwmmWHEJ\n9Js7tIVqnciAf2gGE8WYRDRkqOoxje2hlFbN4MAEiitXIk2bCdim/JBjkOOIMLi0EdAGGGEljRHC\n+EeykSd2uXxdYxKlzSqx3dorZBZ7pe0f52y4Xu1q7949u3v08H7NMNz7nHvP/ej4zOc+e+695wxN\nTU0hSSrT8EoHkCQtHUtekgpmyUtSwSx5SSqYJS9JBbPkJalgq5bzxUZHR/2+piQNYGRkZGiQ5y1r\nyQOMjIw0tq5ut0un02lsfU1qczZod742Z4N25zPb4Nqcb3R0dODnerhGkgpmyUtSwSx5SSqYJS9J\nBbPkJalglrwkFcySl6SCWfKSVLBl/zFUm91+76FZx6/YsG6Zk0hSM5zJS1LBLHlJKpglL0kFs+Ql\nqWDzfvAaEacDnwLOA44B/xyYBHYDU8CDwNbMPB4R24HL6+XXZuZ9SxNbktSPfmbybwRWZeZrgV8F\n/gNwE7AtMy8ChoDNEbEeuATYAGwBdi5NZElSv/op+YeAVRExDKwFngFGgP318r3ApcCFwL7MnMrM\nQ/Vzzl6CzJKkPvXzPfnvUh2q+RbwQuBNwMWZOX2VpyPAmVRvAI/3PG96/NHelXW73cUl7jExMdHo\n+sYOj8863u0eXfC6ms7WtDbna3M2aHc+sw2u7fkG1U/J/zJwR2a+PyLOBf4QWN2zfA3wBDBe3545\n/ixNXnml6Su5HByf/cdQnc7CfwzV5qvMQLvztTkbtDuf2QbX5nxLfWWo/wc8Wd/+v8DpwMGI2FiP\nXQYcAO4CNkXEcESsA4Yz87GBk0mSFq2fmfyHgdsi4gDVDP464I+BXRGxGugCezLzWP2Yu6nePLYu\nUWZJUp/mLfnM/C7w1lkWXTLLY3cAOxadSpLUCH8MJUkFs+QlqWCWvCQVzJKXpIJZ8pJUMEtekgpm\nyUtSwSx5SSqYJS9JBbPkJalglrwkFcySl6SCWfKSVDBLXpIKZslLUsEseUkqmCUvSQWb98pQEXEV\ncFV99wzg1cBG4CPAJLAvMz8QEcPAzcCrgO8Db8/Mh5uPLEnqVz+X/9sN7AaIiJ3AbcAtwFuAR4Av\nR8R64DzgjMx8TURcANwIbF6S1JKkvvR9uCYizgdeCXwWeE5mfjszp4A7gNcDFwJfAcjMe4Dzm48r\nSVqIeWfyPa4DPgCsBcZ7xo8AL6vHn+wZPxYRqzJzsncl3W53wKgnmpiYaHR9Y4fHZx3vdo8ueF1N\nZ2tam/O1ORu0O5/ZBtf2fIPqq+Qj4izgJzLzzohYC6zpWbwGeAJ47ozx4ZkFD9DpdBYR99m63W6j\n6zs4fmjW8U5n3YLX1XS2prU5X5uzQbvzmW1wbc43Ojo68HP7PVxzMfAHAJk5DjwdET8eEUPAJuAA\ncBfwRoD6mPwDA6eSJDWi38M1QfUh67R3Ap8BTqP6ds29EfFHwBsi4hvAEHB1o0klSQvWV8ln5odm\n3L8HuGDG2HGq8pcktYQ/hpKkglnyklQwS16SCmbJS1LBLHlJKpglL0kFs+QlqWCWvCQVzJKXpIJZ\n8pJUMEtekgpmyUtSwSx5SSqYJS9JBbPkJalglrwkFcySl6SC9Xsh7/cDPwesBm4G9gO7gSngQWBr\nZh6PiO3A5cAkcG1m3rcUoSVJ/Zl3Jh8RG4HXAq8DLgHOBW4CtmXmRVTXc90cEevr5RuALcDOJcos\nSepTP4drNgEPAF8Avgh8CRihms0D7AUuBS6kuqj3VGYeAlZFxNnNR5Yk9aufwzUvBH4MeBPwd4Hf\nA4Yzc6pefgQ4E1gLPN7zvOnxR3tX1u12Fxn5ByYmJhpd39jh8VnHu92jC15X09ma1uZ8bc4G7c5n\ntsG1Pd+g+in5x4FvZebTQEbEBNUhm2lrgCeA8fr2zPFn6XQ6g6edodvtNrq+g+OHZh3vdNYteF1N\nZ2tam/O1ORu0O5/ZBtfmfKOjowM/t5/DNV8HfiYihiLiJcDzgP9RH6sHuAw4ANwFbIqI4YhYRzXb\nf2zgZJKkRZt3Jp+ZX4qIi4H7qN4UtgLfAXZFxGqgC+zJzGMRcQC4u+dxkqQV1NdXKDPzvbMMXzLL\n43YAOxYXSZLUFH8MJUkFs+QlqWCWvCQVzJKXpIJZ8pJUMEtekgpmyUtSwSx5SSpYXz+G0tK6/d7Z\nz5lzxYaFnzNHknpZ8n2whCWdqjxcI0kFs+QlqWCWvCQVzJKXpIJZ8pJUMEtekgpmyUtSwfr6nnxE\nHASerO9+B/gE8BFgEtiXmR+IiGHgZuBVwPeBt2fmw81HliT1a96Sj4gzADJzY8/Y/cBbgEeAL0fE\neuA84IzMfE1EXADcCGxegsySpD71M5N/FfDciNhXP34H8JzM/DZARNwBvB44B/gKQGbeExHnL0li\nSVLf+in5p4AbgFuBVwB7gSd6lh8BXgas5QeHdACORcSqzJzsXVm3211U4F4TExONrm/s8PiCHt/t\nHp1z2UKyzfW6J1v/YjW97ZrU5mzQ7nxmG1zb8w2qn5J/CHg4M6eAhyLiSeBv9yxfQ1X6z61vTxue\nWfAAnU5nEXGfrdvtNrq+g+Ozn6NmLp3O3OeuWUi2uV73ZOtfrKa3XZPanA3anc9sg2tzvtHR0YGf\n28+3a66hOr5ORLyEqsyPRsSPR8QQsAk4ANwFvLF+3AXAAwOnkiQ1op+Z/G8BuyPi68AUVekfBz4D\nnEb17Zp7I+KPgDdExDeAIeDqJcosSerTvCWfmU8DV8yy6IIZjzsOvLOhXJKkBvhjKEkqmCUvSQWz\n5CWpYJa8JBXMkpekglnyklQwS16SCmbJS1LBLHlJKpglL0kFs+QlqWCWvCQVzJKXpIJZ8pJUMEte\nkgpmyUtSwfq5MhQR8SJgFHgDMAnsprpK1IPA1sw8HhHbgcvr5ddm5n1LkliS1Ld5Z/IRcTrwCeB7\n9dBNwLbMvIjqMn+bI2I9cAmwAdgC7FyauJKkhejncM0NwC3AX9b3R4D99e29wKXAhVTXep3KzEPA\nqog4u+mwkqSFOWnJR8RVwKOZeUfP8FBmTtW3jwBnAmuBJ3seMz0uSVpB8x2TvwaYiohLgVcDvwO8\nqGf5GuAJYLy+PXP8BN1ud+CwM01MTDS6vrHD4wt6fLd7dM5lC8k21+uebP2L1fS2a1Kbs0G785lt\ncG3PN6iTlnxmXjx9OyK+CrwT+FBEbMzMrwKXAXcCDwMfjIgbgJcCw5n52Gzr7HQ6zSSnesNocn0H\nxw8t6PGdzro5ly0k21yve7L1L1bT265Jbc4G7c5ntsG1Od/o6OjAz+3r2zUzvAfYFRGrgS6wJzOP\nRcQB4G6qQ0BbB04kSWpM3yWfmRt77l4yy/IdwI5FJzqF3H7v7DPwKzYs3QxckhbCH0NJUsEseUkq\nmCUvSQUb5INXzeP2ew8xdnj8hG/NeKxe0nJzJi9JBXMmv4zm+jaOJC0VS77FTvam4KEfSf3wcI0k\nFcySl6SCWfKSVDBLXpIKZslLUsEseUkqmCUvSQWz5CWpYJa8JBXMkpekgs17WoOIOA3YBQRwDLga\nGAJ2A1PAg8DWzDweEduBy4FJ4NrMvG+JckuS+tDPTP5nATLzdcC/A26q/7ctMy+iKvzNEbGe6rKA\nG4AtwM4lSSxJ6tu8JZ+Z/w14R333x4C/AkaA/fXYXuBS4EJgX2ZOZeYhYFVEnN18ZElSv/o6Jp+Z\nkxHxKeBjwB5gKDOn6sVHgDOBtcCTPU+bHpckrZC+TzWcmb8YEe8D7gX+Vs+iNcATwHh9e+b4s3S7\n3cGSzmJiYqLR9Y0dHm9sXZPPPMPY4bHG1jdTt3t0Uc9vets1qc3ZoN35zDa4tucbVD8fvL4NeGlm\n/jrwFHAc+OOI2JiZXwUuA+4EHgY+GBE3AC8FhjPzsZnr63Q6jYXvdruNrm/m5foWY+zwGOe8+JzG\n1jdTp7O488k3ve2a1OZs0O58Zhtcm/ONjo4O/Nx+ZvKfB347Ir4GnA5cC3SBXRGxur69JzOPRcQB\n4G6qw0BbB04lSWrEvCWfmUeBt86y6JJZHrsD2LHoVJKkRvhjKEkqmCUvSQWz5CWpYJa8JBXMkpek\nglnyklQwS16SCmbJS1LBLHlJKpglL0kFs+QlqWCWvCQVzJKXpIJZ8pJUMEtekgpmyUtSwSx5SSrY\nSa8MFRGnA7cB5wHPAX4N+CawG5gCHgS2ZubxiNgOXA5MAtdm5n1LF1uS1I/5ZvJXAo9n5kVUF+z+\nOHATsK0eGwI2R8R6qssBbgC2ADuXLrIkqV/zlfzvAtf33J8ERoD99f29wKXAhcC+zJzKzEPAqog4\nu+mwkqSFOenhmsz8LkBErAH2ANuAGzJzqn7IEeBMYC3weM9Tp8cfnbnObre7+NS1iYmJRtc3dni8\nsXVNPvMMY4fHGlvfTN3u0UU9v+lt16Q2Z4N25zPb4Nqeb1AnLXmAiDgX+AJwc2beHhEf7Fm8BngC\nGK9vzxw/QafTGTztDN1ut9H1HRw/1Ni6xg6Pcc6Lz2lsfTN1OusW9fymt12T2pwN2p3PbINrc77R\n0dGBn3vSwzUR8aPAPuB9mXlbPXwwIjbWty8DDgB3AZsiYjgi1gHDmfnYwKkkSY2YbyZ/HfAC4PqI\nmD42/y7goxGxGugCezLzWEQcAO6meuPYulSBVbn93tn/6rhiw+Jm+JLKMt8x+XdRlfpMl8zy2B3A\njkZSSZIa4Y+hJKlg837wWqK5DnVIUmmcyUtSwSx5SSqYJS9JBbPkJalglrwkFcySl6SCWfKSVDBL\nXpIKZslLUsEseUkqmCUvSQWz5CWpYJa8JBXMkpekglnyklSwvs4nHxEbgN/IzI0R8XJgNzAFPAhs\nzczjEbEduByYBK7NzPuWKLMkqU/zzuQj4r3ArcAZ9dBNwLbMvAgYAjZHxHqqSwJuALYAO5cmriRp\nIfqZyX8b+Hng0/X9EWB/fXsv8NNAAvsycwo4FBGrIuLszHy06cA6OS/wLanXvCWfmZ+LiPN6hobq\nMgc4ApwJrAUe73nM9PgJJd/tdgcOO9PExMRA6xs7PN5YhrlMPvMMY4fHlvx1+tXtHn3W/UG33XJo\nczZodz6zDa7t+QY1yDVej/fcXgM8AYzXt2eOn6DT6QzwkrPrdrsDre/g+NJf43Xs8BjnvPicJX+d\nfnU6z57JD7rtlkObs0G785ltcG3ONzo6OvBzB/l2zcGI2Fjfvgw4ANwFbIqI4YhYBwxn5mMDp5Ik\nNWKQmfx7gF0RsRroAnsy81hEHADupnrj2NpgRknSgPoq+cz8M+CC+vZDVN+kmfmYHcCO5qJJkhbL\nH0NJUsEseUkqmCUvSQUb5IPXFTPzhz5jh8dP+nVIfwAk6YedM3lJKpglL0kFs+QlqWCWvCQV7JT6\n4FWDm+tDaz+clsrmTF6SCuZM/oec55+XyuZMXpIKZslLUsEseUkqWNHH5Oc63ixJPyyKLnkNbqFv\nkH5QK7WTh2skqWCNzuQjYhi4GXgV8H3g7Zn5cJOvoXbyq5hSOzU9k38zcEZmvgb4N8CNDa9fkrQA\nTR+TvxD4CkBm3hMR5ze8fp1iBvnwe77rBPTyLwXp5IampqYaW1lE3Ap8LjP31vcPAS/LzEmA0dHR\n5l5Mkn6IjIyMDA3yvKZn8uPAmp77w9MFD4OHlCQNpulj8ncBbwSIiAuABxpevyRpAZqeyX8BeENE\nfAMYAq5ueP2SpAVo9Jj8UoqIq4Cr6rtnAK8GrgA+BPxFPb49M/cvc64NwG9k5saIeDmwG5gCHgS2\nZubxiNgOXA5MAtdm5n0rlO/VwMeAY1Rfcf1nmflXEfFR4HXAkfppmzPzyWXOth74IvC/6sW/mZn/\nZaW23YxsnwVeXC86D7gnM7dExO8BPwI8A3wvMy9bhlynA7fVOZ4D/BrwTVqw382R7RAt2efmyPe/\nacF+N0e2K2hgvztlfvGambupdmQiYifVBlkPvDczP7cSmSLivcDbgKP10E3Atsz8akTcAmyOiD8H\nLgE2AOcCnwN+coXyfQT4pcy8PyL+BfA+4N1U23FTZj62HLnmyLYeuCkzb+x5zHpWYNvNzJaZW+rx\nFwB3Ar9cP/TlwCszczlnSlcCj2fm2yLiR4CDwP20Y7+bLdt3aMk+N0e+X6Ud+90J2TJzXZ1nUfvd\nKfeL1/prma/MzE8CI8A1EXEgIm6MiOV+0/o28PM990eA6b8k9gKXUn2tdF9mTmXmIWBVRJy9Qvm2\nZOb99e1VwET9A7ZXAJ+MiLsi4poVyjYCXB4RX4uI34qINazctpuZbdoHgI9l5lhE/ChwFvDFiPh6\nRLxpGXIB/C5wfc/9Sdqz382WrU373Fzbrg373WzZpi1qvzvlSh64juofDfD7wC8BFwPPB965nEHq\nvyCe6Rka6nl3PQKcCawFev8MnR5f9nyZOQYQEa8F/jXwYeB5VH9OXwn8DPCvIuIfLnc24D7gVzLz\nYuARYDsrtO1myUZEvAh4PfVfk8Bqqh/7vZnqDeHD9WOWOtt3M/NIXUZ7gG20ZL+bLVvL9rnZtl0r\n9rs5sjWy351SJR8RZwE/kZl31kO3ZeYj9Q7+34F/tHLpADjec3sN8AQnfq10enxFRMQ/AW4BLs/M\nR4GngI9k5lOZeQT4Q6rTUiy3L2Tm6PRtqv+Wbdp2vwDcnpnH6vuHgVsyczIz/5rqT/9YjiARcS7V\nn++fzszbadF+N0u2Vu1zs+RrzX4327ajgf3ulCp5qhn7HwBExBDwpxHx0nrZ64HRuZ64TA5GxMb6\n9mXAAaqvlW6KiOGIWEf124HlPA75NyLiSqrZ1MbMfKQe/nvA1yPitPrDnwuBP1mBeHdExE/Vt6f/\nW7Zm21EdAtk74/5/BYiI5wN/H+gudYj6z/V9wPsy87Z6uBX73WzZ2rTPzbHtWrHfzZENGtjvTpkP\nXmtB9ScVmTkVEW8HPh8R36P6hsGulQwHvAfYFRGrqTb8nsw8FhEHgLup3lS3rkSwiDgN+CjVtx0+\nHxEA+zNze0R8BriH6hDF72Tm/1yBiP8S+HhEPE01W3lHZo63YdvV/mbfA8jMvRGxKSLuoZpJX7dM\nb0DXAS8Aro+I6WO47wI+2oL9bma206hK6M9pxz4327Z7N/CfWrDfzZbtMhrY706Zr1BKkhbuVDtc\nI0laAEtekgpmyUtSwSx5SSqYJS9JBbPkpVlExMaIeHClc0iLZclLUsFOtR9DSc8SEbuAv87Mf1vf\nvxJ4S2b+457H/DRwY2b+g/r+WVRnR3wZ1elur6M6J8iLgE9l5vUzXmM38GBm3jDzfkT8HeDjwDrg\ndOCzmfkfl+5fLC2MM3md6nYCV/ecgfQdVOdJ6fX7wPPjBxeW/6fAl6nOR/Ie4Bcz83zgAuD9EfHC\nBbz+p6nOoTQC/BRwaUS8dbB/itQ8S16ntPo0tt+hOl1sB3gJ1TlAeh8zRXX9gavqoauBXfX4zwIj\nUV0k4iaqK5o9r5/XjojnUZ13/N9HxP1UP9NfR3VBG6kVPFyjEuwErgEeAj45x8UUbgP+JCJuBc7K\nzP11SR+kOvvggfoxb6Yq+l5TM8ZW1/9/Wj3+2sx8CqD+K2CikX+V1ABn8irBHqpTxP4CVVGfIDP/\nD9W5wz8B3FoPv4Lq3OHbMvOLwEaqS6+dNuPpjwLnA0TES6hm72TmONXs/d31srOozmC4uZl/lrR4\nlrxOeZn5NFXRf2OeM/Ltonoz+FR9/0+BLwHfiogu1aGbb1JdXq3Xx4BzIiKB36Y6//m0K4ALIuIB\n4F7gP2fmZxb5T5Ia41kodcqrD7t8jeoC1vesdB6pTZzJ65QWEZuAvwD2WvDSiZzJS1LBnMlLUsEs\neUkqmCUvSQWz5CWpYJa8JBXMkpekgv1/r1lx9jrYidMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xcdffef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.distplot(data_train.y.values, bins=50, kde=False)\n",
    "plt.xlabel('y value', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX8AAAEFCAYAAAAL/efAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n1hSoLqvKJwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xcdff978>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#on va voir la réaction de la variable Y\n",
    "plt.scatter(range(data_train.shape[0]), np.sort(data_train.y.values))\n",
    "plt.xlabel('voiture n°', fontsize=12)\n",
    "plt.ylabel('y', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXkAAAECCAYAAAAb5qc/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\neUkqmCUvSQWz5CWpYJa8JBXMkpekgv1/r1lx9jrYidMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xd661470>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "sns.distplot(data_train.y.values, bins=50, kde=False)\n",
    "plt.xlabel('y value', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    3338\n",
       "1     871\n",
       "Name: X300, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train['X300'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Données pour le test \n",
    "data_test = pd.read_csv(\"data/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4209, 377)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dimension du frame \n",
    "data_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#data_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "z     360\n",
       "ak    349\n",
       "y     324\n",
       "ay    313\n",
       "t     306\n",
       "x     300\n",
       "o     269\n",
       "f     227\n",
       "n     195\n",
       "w     182\n",
       "j     181\n",
       "az    175\n",
       "aj    151\n",
       "s     106\n",
       "ap    103\n",
       "h      75\n",
       "d      73\n",
       "al     67\n",
       "v      36\n",
       "af     35\n",
       "m      34\n",
       "ai     34\n",
       "e      32\n",
       "ba     27\n",
       "at     25\n",
       "a      21\n",
       "ax     19\n",
       "aq     18\n",
       "i      18\n",
       "am     18\n",
       "u      17\n",
       "aw     16\n",
       "l      16\n",
       "ad     14\n",
       "b      11\n",
       "k      11\n",
       "au     11\n",
       "as     10\n",
       "r      10\n",
       "bc      6\n",
       "ao      4\n",
       "c       3\n",
       "aa      2\n",
       "q       2\n",
       "ac      1\n",
       "g       1\n",
       "ab      1\n",
       "Name: X0, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#exploration de la variable X0\n",
    "data_train['X0'].value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "unique_values_dict = {}\n",
    "for col in data_train.columns:\n",
    "    if col not in [\"ID\", \"y\", \"X0\", \"X1\", \"X2\", \"X3\", \"X4\", \"X5\", \"X6\", \"X8\"]:\n",
    "        unique_value = str(np.sort(train_df[col].unique()).tolist())\n",
    "        tlist = unique_values_dict.get(unique_value, [])\n",
    "        tlist.append(col)\n",
    "        unique_values_dict[unique_value] = tlist[:]\n",
    "for unique_val, columns in unique_values_dict.items():\n",
    "    print(\"Columns containing the unique values : \",unique_val)\n",
    "    print(columns)\n",
    "    print(\"--------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Extrere les colonnes des deux variables X10, X380\n",
    "#data_train.ix[:,[\"X10\",\"X380\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test=data_train.as_matrix()\n",
    "#print(test[:,10:385])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#transfomer les objets en code numérique \n",
    "\n",
    "for c in data_train:\n",
    "    if data_train[c].dtype == 'object':\n",
    "        lbl = LabelEncoder()\n",
    "        lbl.fit(list(data_train[c].values)+list(data_test[c].values))\n",
    "        data_train[c] = lbl.transform(list(data_train[c].values))\n",
    "        data_test[c]= lbl.transform(list(data_test[c].values))\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4209 entries, 0 to 4208\n",
      "Columns: 378 entries, ID to X385\n",
      "dtypes: float64(1), int64(377)\n",
      "memory usage: 12.1 MB\n"
     ]
    }
   ],
   "source": [
    "data_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4209 entries, 0 to 4208\n",
      "Columns: 377 entries, ID to X385\n",
      "dtypes: int64(377)\n",
      "memory usage: 12.1 MB\n"
     ]
    }
   ],
   "source": [
    "data_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "Y_train= data_train[\"y\"].values\n",
    "X_train= data_train.drop([\"y\",\"ID\"], axis=1)\n",
    "X_test = data_test.drop([\"ID\"], axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4209 entries, 0 to 4208\n",
      "Columns: 377 entries, ID to X385\n",
      "dtypes: int64(377)\n",
      "memory usage: 12.1 MB\n"
     ]
    }
   ],
   "source": [
    "data_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4209 entries, 0 to 4208\n",
      "Columns: 376 entries, X0 to X385\n",
      "dtypes: int64(376)\n",
      "memory usage: 12.1 MB\n"
     ]
    }
   ],
   "source": [
    "X_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data_train[\"eval_set\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Train the xgb model then predict the test data\n",
    "\n",
    "model_1 = XGBRegressor(learning_rate=0.01, n_estimators=400, subsample=0.8, \n",
    "                      colsample_bytree=0.8, max_depth =10)\n",
    "model_1.fit(X_train, Y_train)\n",
    "\n",
    "Y_predict = model_1.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anacanda\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=6.025e-02, with an active set of 2 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\anacanda\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=5.597e-02, with an active set of 3 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\anacanda\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=5.597e-02, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\anacanda\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=3.795e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\anacanda\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.969e-02, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\anacanda\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=2.755e-02, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\anacanda\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=2.755e-02, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\anacanda\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=1.846e-02, with an active set of 12 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\anacanda\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=1.402e-02, with an active set of 14 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\anacanda\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=1.402e-02, with an active set of 14 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\anacanda\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=1.367e-02, with an active set of 16 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\anacanda\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=1.354e-02, with an active set of 17 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\anacanda\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=1.354e-02, with an active set of 17 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\anacanda\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=1.239e-02, with an active set of 17 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\anacanda\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=1.194e-02, with an active set of 18 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\anacanda\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=1.184e-02, with an active set of 18 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\anacanda\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:339: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 25 iterations, alpha=1.173e-02, previous alpha=1.038e-02, with an active set of 22 regressors.\n",
      "  ConvergenceWarning)\n",
      "C:\\anacanda\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=5.893e-02, with an active set of 2 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\anacanda\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=3.462e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\anacanda\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=3.462e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\anacanda\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=2.886e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\anacanda\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=2.098e-02, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\anacanda\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=1.731e-02, with an active set of 13 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\anacanda\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=1.708e-02, with an active set of 13 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\anacanda\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=1.524e-02, with an active set of 14 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\anacanda\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=1.512e-02, with an active set of 14 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\anacanda\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=1.420e-02, with an active set of 16 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\anacanda\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=1.170e-02, with an active set of 18 regressors, and the smallest cholesky pivot element being 4.942e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\anacanda\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=1.170e-02, with an active set of 18 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\anacanda\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:339: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 22 iterations, alpha=1.137e-02, previous alpha=1.136e-02, with an active set of 19 regressors.\n",
      "  ConvergenceWarning)\n",
      "C:\\anacanda\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=5.688e-02, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\anacanda\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=2.844e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\anacanda\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.933e-02, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\anacanda\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=1.495e-02, with an active set of 15 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\anacanda\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=1.395e-02, with an active set of 16 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\anacanda\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=1.219e-02, with an active set of 18 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\anacanda\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=1.133e-02, with an active set of 19 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\anacanda\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=7.179e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\anacanda\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=7.179e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\anacanda\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=6.943e-03, with an active set of 32 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\anacanda\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:339: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 37 iterations, alpha=6.873e-03, previous alpha=6.805e-03, with an active set of 34 regressors.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anacanda\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=4.682e-02, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\anacanda\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=4.682e-02, with an active set of 3 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\anacanda\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=2.341e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\anacanda\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=2.341e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\anacanda\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=1.330e-02, with an active set of 14 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\anacanda\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=1.282e-02, with an active set of 15 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\anacanda\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=1.211e-02, with an active set of 16 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\anacanda\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=1.210e-02, with an active set of 17 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\anacanda\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=1.124e-02, with an active set of 20 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\anacanda\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=1.124e-02, with an active set of 20 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\anacanda\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:339: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 22 iterations, alpha=1.116e-02, previous alpha=1.094e-02, with an active set of 21 regressors.\n",
      "  ConvergenceWarning)\n",
      "C:\\anacanda\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=7.646e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\anacanda\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=6.603e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\anacanda\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=3.743e-03, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\anacanda\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=3.215e-03, with an active set of 32 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\anacanda\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 37 iterations, i.e. alpha=3.060e-03, with an active set of 37 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\anacanda\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=3.054e-03, with an active set of 38 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\anacanda\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 49 iterations, i.e. alpha=2.630e-03, with an active set of 49 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\anacanda\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 52 iterations, i.e. alpha=2.577e-03, with an active set of 52 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\anacanda\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 54 iterations, i.e. alpha=2.555e-03, with an active set of 54 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\anacanda\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 54 iterations, i.e. alpha=2.555e-03, with an active set of 54 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\anacanda\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 62 iterations, i.e. alpha=2.291e-03, with an active set of 62 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\anacanda\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:339: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 69 iterations, alpha=2.242e-03, previous alpha=2.220e-03, with an active set of 68 regressors.\n",
      "  ConvergenceWarning)\n",
      "C:\\anacanda\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=7.917e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\anacanda\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=4.068e-03, with an active set of 24 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\anacanda\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=3.515e-03, with an active set of 29 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\anacanda\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=3.447e-03, with an active set of 32 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\anacanda\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=3.447e-03, with an active set of 32 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\anacanda\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=3.447e-03, with an active set of 32 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\anacanda\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=3.397e-03, with an active set of 34 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\anacanda\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=3.136e-03, with an active set of 39 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\anacanda\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=2.928e-03, with an active set of 44 regressors, and the smallest cholesky pivot element being 3.161e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\anacanda\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 54 iterations, i.e. alpha=2.508e-03, with an active set of 54 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\anacanda\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 54 iterations, i.e. alpha=2.508e-03, with an active set of 54 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\anacanda\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 71 iterations, i.e. alpha=1.970e-03, with an active set of 71 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anacanda\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 71 iterations, i.e. alpha=1.970e-03, with an active set of 71 regressors, and the smallest cholesky pivot element being 4.829e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\anacanda\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 77 iterations, i.e. alpha=1.758e-03, with an active set of 77 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\anacanda\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 77 iterations, i.e. alpha=1.758e-03, with an active set of 77 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\anacanda\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 83 iterations, i.e. alpha=1.633e-03, with an active set of 83 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\anacanda\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 84 iterations, i.e. alpha=1.630e-03, with an active set of 84 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\anacanda\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 84 iterations, i.e. alpha=1.630e-03, with an active set of 84 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\anacanda\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 92 iterations, i.e. alpha=1.494e-03, with an active set of 92 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\anacanda\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 100 iterations, i.e. alpha=1.422e-03, with an active set of 98 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\anacanda\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 100 iterations, i.e. alpha=1.422e-03, with an active set of 98 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\anacanda\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:339: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 104 iterations, alpha=1.509e-03, previous alpha=1.357e-03, with an active set of 101 regressors.\n",
      "  ConvergenceWarning)\n",
      "C:\\anacanda\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=7.299e-03, with an active set of 9 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\anacanda\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=3.835e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\anacanda\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=3.787e-03, with an active set of 32 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\anacanda\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=3.787e-03, with an active set of 32 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\anacanda\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=3.787e-03, with an active set of 32 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\anacanda\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=3.598e-03, with an active set of 35 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\anacanda\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=3.598e-03, with an active set of 35 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\anacanda\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=3.383e-03, with an active set of 42 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\anacanda\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=3.083e-03, with an active set of 45 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\anacanda\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=3.083e-03, with an active set of 45 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\anacanda\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 48 iterations, i.e. alpha=2.995e-03, with an active set of 46 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\anacanda\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 55 iterations, i.e. alpha=2.693e-03, with an active set of 53 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\anacanda\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 57 iterations, i.e. alpha=2.672e-03, with an active set of 55 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\anacanda\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 59 iterations, i.e. alpha=2.652e-03, with an active set of 57 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\anacanda\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 65 iterations, i.e. alpha=2.281e-03, with an active set of 63 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\anacanda\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 65 iterations, i.e. alpha=2.281e-03, with an active set of 63 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\anacanda\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 68 iterations, i.e. alpha=2.186e-03, with an active set of 66 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\anacanda\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 79 iterations, i.e. alpha=1.857e-03, with an active set of 77 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\anacanda\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 79 iterations, i.e. alpha=1.857e-03, with an active set of 77 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\anacanda\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 81 iterations, i.e. alpha=1.825e-03, with an active set of 79 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\anacanda\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 81 iterations, i.e. alpha=1.825e-03, with an active set of 79 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\anacanda\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 81 iterations, i.e. alpha=1.825e-03, with an active set of 79 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\anacanda\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 82 iterations, i.e. alpha=1.809e-03, with an active set of 80 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\anacanda\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 82 iterations, i.e. alpha=1.809e-03, with an active set of 80 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\anacanda\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 84 iterations, i.e. alpha=1.775e-03, with an active set of 82 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\anacanda\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 85 iterations, i.e. alpha=1.775e-03, with an active set of 83 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\anacanda\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 92 iterations, i.e. alpha=1.653e-03, with an active set of 88 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\anacanda\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 94 iterations, i.e. alpha=1.603e-03, with an active set of 90 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\anacanda\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 94 iterations, i.e. alpha=1.598e-03, with an active set of 90 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\anacanda\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 95 iterations, i.e. alpha=1.563e-03, with an active set of 91 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\anacanda\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:339: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 98 iterations, alpha=1.580e-03, previous alpha=1.529e-03, with an active set of 93 regressors.\n",
      "  ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "#utilisation du GradientBoostingRegressor\n",
    "stacked_pipeline = make_pipeline(\n",
    "    StackingEstimator(estimator=LassoLarsCV(normalize=True)),\n",
    "    StackingEstimator(estimator=GradientBoostingRegressor(learning_rate=0.001, loss=\"huber\", max_depth=3, max_features=0.55, min_samples_leaf=18, min_samples_split=14, subsample=0.7)),\n",
    "    LassoLarsCV()\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "stacked_pipeline.fit(X_train, Y_train)\n",
    "results = stacked_pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anacanda\\lib\\site-packages\\sklearn\\grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n",
      "C:\\anacanda\\lib\\site-packages\\sklearn\\learning_curve.py:22: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the functions are moved. This module will be removed in 0.20\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  79.56459808,   96.78109741,   80.06500244, ...,   91.22626495,\n",
       "        107.25185394,   89.81809235], dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99.021088"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_predict.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#prediction du temps de test pour chaque ID \n",
    "id_test = data_test['ID'].values\n",
    "sub = pd.DataFrame()\n",
    "sub['ID'] = id_test\n",
    "sub['y'] = Y_predict\n",
    "sub.to_csv('xgbpred_model.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "id_test = data_test['ID'].values\n",
    "sub = pd.DataFrame()\n",
    "sub['ID'] = id_test\n",
    "sub['y'] = Y_predict*0.6 + results*0.4\n",
    "sub.to_csv('xgbpred&stacked_pipeline.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#GradientBoostingRegressor\n",
    "id_test = data_test['ID'].values\n",
    "sub = pd.DataFrame()\n",
    "sub['ID'] = id_test\n",
    "sub['y'] = results\n",
    "sub.to_csv('GradientBoostingRegressor.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score R2 pour le model 3\n",
      "0.577474349126\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#R2 Score on the entire Train data when averaging'''\n",
    "\n",
    "print('score R2 pour le model 3')\n",
    "print(r2_score(Y_train,stacked_pipeline.predict(X_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score R2 pour le model 1\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-42-66b0746bf1dd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'score R2 pour le model 1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr2_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "#R2 Score on the entire Train data when averaging'''\n",
    "\n",
    "print('score R2 pour le model 1')\n",
    "print(r2_score(Y_train,model.predict(X_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
